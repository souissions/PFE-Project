# ðŸ§  RAG â€“ Medical Retrieval-Augmented Generation System

This project implements a custom **RAG (Retrieval-Augmented Generation)** system for intelligent document understanding and question answering. It's built using:

- âš™ï¸ **FastAPI** for backend APIs
- ðŸ§  **Cohere** for both embeddings & LLM answer generation
- ðŸ“Š **PGVector** as the vector similarity database
- ðŸŽ›ï¸ **Streamlit** interface for file upload and user interaction

---

## ðŸ”§ Features

- âœ… Upload `.txt` or `.pdf` medical documents
- âœ… Automatic chunking of text into digestible segments
- âœ… Generation of **384-dimensional embeddings** using Cohere
- âœ… Storage of vectors in **PGVector** for semantic retrieval
- âœ… LLM-powered answer generation based on query + retrieved context

---

## ðŸ“ Project Structure

```
mini-rag/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py                  # FastAPI app entry point
â”‚   â”œâ”€â”€ routes/                  # API endpoints (data, NLP)
â”‚   â”œâ”€â”€ controllers/             # Business logic (file processing, indexing, search)
â”‚   â”œâ”€â”€ models/                  # DB models + ORM layer
â”‚   â”œâ”€â”€ stores/
â”‚   â”‚   â”œâ”€â”€ llm/                 # Embedding + generation (Cohere/OpenAI)
â”‚   â”‚   â””â”€â”€ vectordb/            # PGVector or Qdrant integration
â”‚   â”œâ”€â”€ views/                   # Streamlit frontend
â”‚
â”œâ”€â”€ docker/                      # Docker for PGVector/Mongo setup
â”œâ”€â”€ .env.example                 # Sample env config
â”œâ”€â”€ requirements.txt             # Python dependencies
â””â”€â”€ README.md
```

---

## âš™ï¸ Setup & Installation

### 1. Install dependencies

```bash
pip install -r requirements.txt
```

---

### 2. Create `.env` file

```bash
cp .env.example .env
```

Then fill in the values for:
- `COHERE_API_KEY`
- `POSTGRES_USERNAME`, `POSTGRES_PASSWORD`, `POSTGRES_PORT`, etc.
- `GENERATION_MODEL_ID` (e.g., `command-a-03-2025`)
- `EMBEDDING_MODEL_ID` (e.g., `embed-multilingual-light-v3.0`)

---

### 3. Run Alembic DB Migration

```bash
alembic upgrade head
```

This sets up the DB tables for projects, assets, and chunks.

---

## ðŸ³ Start Vector DB via Docker

```bash
cd docker
cp .env.example .env  # optional but recommended
docker compose up -d
```

---

## ðŸš€ Run the FastAPI Server

```bash
uvicorn main:app --reload --host 0.0.0.0 --port 5000
```

Go to:  
ðŸ“„ Swagger API Docs: http://localhost:5000/docs  
Use this interface to test all endpoints: upload, process, index, search, and get answers

---

## ðŸ–¼ï¸ Streamlit App (File Uploader & QA)

```bash
streamlit run src/views/file_upload_app.py
```

> Upload a medical document, and it will be chunked, embedded, indexed, and ready for querying.

---

## ðŸ”Ž RAG Workflow Overview

1. **Upload File** â†’ TXT / PDF  
2. **Chunk Text** â†’ via custom splitter  
3. **Generate Embeddings** â†’ using Cohereâ€™s multilingual model  
4. **Store Vectors** â†’ in PGVector (Postgres)  
5. **User Query** â†’ embedded and compared via **cosine similarity**  
6. **Retrieve Top Chunks** â†’ most relevant context  
7. **LLM Prompt + Answer** â†’ using Cohere LLM (`command-a-03-2025`)  

---

## ðŸ“¦ API Endpoints (Examples)

| Endpoint                            | Description                           |
|-------------------------------------|---------------------------------------|
| `POST /api/v1/data/upload/{id}`     | Uploads a document                    |
| `POST /api/v1/data/process/{id}`    | Chunks & embeds document              |
| `POST /api/v1/nlp/index/push/{id}`  | Indexes all chunks into PGVector      |
| `POST /api/v1/nlp/index/search/{id}`| Search relevant chunks via query      |
| `POST /api/v1/nlp/index/answer/{id}`| Generates an LLM answer               |

---

## ðŸ§  Models Used

| Purpose     | Model                                  |
|-------------|----------------------------------------|
| Embeddings  | `embed-multilingual-light-v3.0`(Cohere)|
| Generation  | `command-a-03-2025` (Cohere)           |

Embeddings have **384 dimensions** representing text meaning in vector space.  
Similarity is calculated using **cosine similarity**.



